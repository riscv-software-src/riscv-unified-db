%version: 1.0

include "builtin_functions.idl"
include "util.idl"
include "fp.idl"

# global state

# general purpose register file
# is BUILTIN storage.
# This is to accommodate the zero register (x0) without
# needed ISL language support or hard-to-read function calls
# on any x register read/write
# Bits<XLEN> X[31];

############################################################
# constants used to indicate special values
############################################################

# CSR field value is undefined, but whatever it is, it must be legal for the field
Bits<65> UNDEFINED_LEGAL = 65'h10000000000000000;

# CSR field value is undefined, but whateer it is, it must be legal for the field and it
# must be the same value if the same sequence of instructions leading to the read is executed
Bits<66> UNDEFINED_LEGAL_DETERMINISTIC = 66'h20000000000000000;

# Signals an illegal write of a WLRL field
Bits<67> ILLEGAL_WLRL = 67'h40000000000000000;

# encoded as defined in the privilege spec
enum PrivilegeMode {
  M  0b011
  S  0b001
  HS 0b001 # alias for S when H extension is used
  U  0b000
  VS 0b101
  VU 0b100
}

enum MemoryOperation {
  Read
  Write
  ReadModifyWrite
  Fetch
}

# Types of Atmoic Read-modify-write operations
enum AmoOperation {
  Swap
  Add
  And
  Or
  Xor
  Max
  Maxu
  Min
  Minu
}

enum PmaAttribute {
  RsrvNone         # LR/SC not allowed
  RsrvNonEventual  # LR/SC allowed, but no gaurantee it will ever succeed
  RsrvEventual     # LR/SC with forward-progress gaurantees

  MAG16            # Misaligned Atomicity Granule = 16-byte
  MAG8             # Misaligned Atomicity Granule = 8-byte
  MAG4             # Misaligned Atomicity Granule = 4-byte
  MAG2             # Misaligned Atomicity Granule = 2-byte

  AmoNone          # No AMOs allowed
  AmoSwap          # amoswap is allowed
  AmoLogical       # amoswap, amoand, amoor, and amoxor are allowed
  AmoArithmetic    # All amos are allowed

  # page walk permissions
  HardwarePageTableRead    # permission to read during a page walk
  HardwarePageTableWrite   # permission to write during a page walk
}

# do not change these values!! the compiler assumes them
enum CsrFieldType {
  RO   0
  ROH  1
  RW   2
  RWR  3
  RWH  4
  RWRH 5
}

# generated from extension information in arch defintion
builtin enum ExtensionName;

# generated from extension information in arch defintion
builtin enum InterruptCode;

# generated from extension information in arch defintion
builtin enum ExceptionCode;

# XLEN encoding, as defined in CSR[mstatus].mxl, etc.
enum XRegWidth {
  XLEN32 0
  XLEN64 1
}

# enum ExceptionCode {
#   None 0xffff
#   InstructionAddressMisaligned 0
#   InstructionAccessFault 1
#   IllegalInstruction 2
#   Breakpoint 3
#   LoadAddressMisaligned 4
#   LoadAccessFault 5
#   StoreAmoAddressMisaligned 6
#   StoreAmoAccessFault 7
#   Ucall 8
#   Scall 9
#   # reserved 10
#   Mcall 11
#   InstructionPageFault 12
#   LoadPageFault 13
#   # reserved 14
#   StoreAmoPageFault 15
#   # reserved 16-17
#   SoftwareCheck 18
#   HardwareError 19
#   InstructionGuestPageFault 20
#   LoadGuestPageFault 21
#   VirtualInstruction 22
#   StoreAmoGuestPageFault 23
# }

enum SatpMode {
  Bare 0
  Sv32 1
  Sv39 8
  Sv48 9
  Sv57 10
}

bitfield (10) PteFlags {
  RSW  9-8 # reserved
  D 7 # dirty
  A 6 # access
  G 5 # global
  U 4 # userspace permission
  X 3 # execute permission
  W 2 # write permission
  R 1 # read permission
  V 0 # valid
}

bitfield (64) Sv39PageTableEntry {
  N 63
  PBMT 62-61
  Reserved 60-54
  PPN2 53-28
  PPN1 27-19
  PPN0 18-10
  PPN 53-10 # in addition to the components, we define the entire PPN
  RSW  9-8
  D 7
  A 6
  G 5
  U 4
  X 3
  W 2
  R 1
  V 0
}

PrivilegeMode current_mode = PrivilegeMode::M;

function mode {
  returns PrivilegeMode
  description {
    Returns the current active privilege mode.
  }
  body {
    if (!implemented?(ExtensionName::S) &&
        !implemented?(ExtensionName::U) &&
        !implemented?(ExtensionName::H)) {
      return PrivilegeMode::M;
    } else {
      return current_mode;
    }
  }
}


function set_mode {
  arguments PrivilegeMode new_mode
  description {
    Set the current privilege mode to `new_mode`
  }
  body {
    if (new_mode != current_mode) {
      notify_mode_change(new_mode, current_mode);
      current_mode = new_mode;
    }
  }
}

function exception_handling_mode {
  returns PrivilegeMode
  arguments ExceptionCode exception_code
  description {
    Returns the target privilege mode that will handle synchronous exception `exception_code`
  }
  body {
    if (mode() == PrivilegeMode::M) {
      # exceptions can never be taken in a less-privileged mode, so if the current
      # mode is M, the value of medeleg is irrelevant
      return PrivilegeMode::M;
    } else if (implemented?(ExtensionName::S) && ((mode() == PrivilegeMode::HS) || (mode() == PrivilegeMode::U))) {
      if ((CSR[medeleg] & (1 << $bits(exception_code))) != 0) {
        return PrivilegeMode::HS;
      } else {
        return PrivilegeMode::M;
      }
    } else {
      assert(implemented?(ExtensionName::H) && ((mode() == PrivilegeMode::VS) || (mode() == PrivilegeMode::VU)), "Unexpected mode");
      if ((CSR[medeleg] &  (1 << $bits(exception_code))) != 0) {
        if ((CSR[hedeleg] & (1 << $bits(exception_code))) != 0) {
          return PrivilegeMode::VS;
        } else {
          return PrivilegeMode::HS;
        }
      } else {
        # if an exception is not delegated to HS-mode, it can't be delegated to VS-mode
        return PrivilegeMode::M;
      }
    }
  }
}

function mtval_readonly? {
  returns Boolean
  description {
    Returns whether or not CSR[mtval] is read-only based on implementation options
  }
  body {
    return !(
      REPORT_VA_IN_MTVAL_ON_BREAKPOINT ||
      REPORT_VA_IN_MTVAL_ON_LOAD_MISALIGNED ||
      REPORT_VA_IN_MTVAL_ON_STORE_MISALIGNED ||
      (implemented?(ExtensionName::Zaamo) && REPORT_VA_IN_MTVAL_ON_AMO_MISALIGNED) ||
      REPORT_VA_IN_MTVAL_ON_INSTRUCTION_MISALIGNED ||
      REPORT_VA_IN_MTVAL_ON_LOAD_ACCESS_FAULT ||
      REPORT_VA_IN_MTVAL_ON_STORE_ACCESS_FAULT ||
      (implemented?(ExtensionName::Zaamo) && REPORT_VA_IN_MTVAL_ON_AMO_ACCESS_FAULT) ||
      REPORT_VA_IN_MTVAL_ON_INSTRUCTION_ACCESS_FAULT ||
      REPORT_VA_IN_MTVAL_ON_LOAD_PAGE_FAULT ||
      REPORT_VA_IN_MTVAL_ON_STORE_PAGE_FAULT ||
      (implemented?(ExtensionName::Zaamo) && REPORT_VA_IN_MTVAL_ON_AMO_PAGE_FAULT) ||
      REPORT_VA_IN_MTVAL_ON_INSTRUCTION_PAGE_FAULT ||
      REPORT_ENCODING_IN_MTVAL_ON_ILLEGAL_INSTRUCTION ||
      # REPORT_CAUSE_IN_MTVAL_ON_SOFTWARE_CHECK ||
      implemented?(ExtensionName::Sdext)
    );
  }
}

function stval_readonly? {
  returns Boolean
  description {
    Returns whether or not CSR[stval] is read-only based on implementation options
  }
  body {
    if (implemented?(ExtensionName::S)) {
      return !(
        REPORT_VA_IN_STVAL_ON_BREAKPOINT ||
        REPORT_VA_IN_STVAL_ON_LOAD_MISALIGNED ||
        REPORT_VA_IN_STVAL_ON_STORE_AMO_MISALIGNED ||
        REPORT_VA_IN_STVAL_ON_INSTRUCTION_MISALIGNED ||
        REPORT_VA_IN_STVAL_ON_LOAD_ACCESS_FAULT ||
        REPORT_VA_IN_STVAL_ON_STORE_AMO_ACCESS_FAULT ||
        REPORT_VA_IN_STVAL_ON_INSTRUCTION_ACCESS_FAULT ||
        REPORT_VA_IN_STVAL_ON_LOAD_PAGE_FAULT ||
        REPORT_VA_IN_STVAL_ON_STORE_AMO_PAGE_FAULT ||
        REPORT_VA_IN_STVAL_ON_INSTRUCTION_PAGE_FAULT ||
        REPORT_ENCODING_IN_STVAL_ON_ILLEGAL_INSTRUCTION ||
        REPORT_CAUSE_IN_STVAL_ON_SOFTWARE_CHECK ||
        implemented?(ExtensionName::Sdext)
      );
    } else {
      return true;
    }
  }
}

function vstval_readonly? {
  returns Boolean
  description {
    Returns whether or not CSR[vstval] is read-only based on implementation options
  }
  body {
    if (implemented?(ExtensionName::H)) {
      return !(
        REPORT_VA_IN_VSTVAL_ON_BREAKPOINT ||
        REPORT_VA_IN_VSTVAL_ON_LOAD_MISALIGNED ||
        REPORT_VA_IN_VSTVAL_ON_STORE_AMO_MISALIGNED ||
        REPORT_VA_IN_VSTVAL_ON_INSTRUCTION_MISALIGNED ||
        REPORT_VA_IN_VSTVAL_ON_LOAD_ACCESS_FAULT ||
        REPORT_VA_IN_VSTVAL_ON_STORE_AMO_ACCESS_FAULT ||
        REPORT_VA_IN_VSTVAL_ON_INSTRUCTION_ACCESS_FAULT ||
        REPORT_VA_IN_VSTVAL_ON_LOAD_PAGE_FAULT ||
        REPORT_VA_IN_VSTVAL_ON_STORE_AMO_PAGE_FAULT ||
        REPORT_VA_IN_VSTVAL_ON_INSTRUCTION_PAGE_FAULT ||
        REPORT_ENCODING_IN_VSTVAL_ON_ILLEGAL_INSTRUCTION ||
        REPORT_CAUSE_IN_VSTVAL_ON_SOFTWARE_CHECK ||
        implemented?(ExtensionName::Sdext)
      );
    } else {
      return true;
    }
  }
}

function mtval_for {
  returns XReg
  arguments ExceptionCode exception_code, XReg tval
  description {
    Given an exception code and a *legal* non-zero value for mtval,
    returns the value to be written in mtval considering implementation options
  }
  body {
    if (exception_code == ExceptionCode::Breakpoint) {
      return REPORT_VA_IN_MTVAL_ON_BREAKPOINT ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadAddressMisaligned) {
      return REPORT_VA_IN_MTVAL_ON_LOAD_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoAddressMisaligned) {
      return REPORT_VA_IN_MTVAL_ON_STORE_AMO_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionAddressMisaligned) {
      return REPORT_VA_IN_MTVAL_ON_INSTRUCTION_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadAccessFault) {
      return REPORT_VA_IN_MTVAL_ON_LOAD_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoAccessFault) {
      return REPORT_VA_IN_MTVAL_ON_STORE_AMO_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionAccessFault) {
      return REPORT_VA_IN_MTVAL_ON_INSTRUCTION_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadPageFault) {
      return REPORT_VA_IN_MTVAL_ON_LOAD_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoPageFault) {
      return REPORT_VA_IN_MTVAL_ON_STORE_AMO_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionPageFault) {
      return REPORT_VA_IN_MTVAL_ON_INSTRUCTION_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::IllegalInstruction) {
      return REPORT_ENCODING_IN_MTVAL_ON_ILLEGAL_INSTRUCTION ? tval : 0;
    # } else if (exception_code == ExceptionCode::SoftwareCheck) {
      # return REPORT_CAUSE_IN_MTVAL_ON_SOFTWARE_CHECK ? tval : 0;
    } else {
      return 0;
    }
  }
}

function stval_for {
  returns XReg
  arguments ExceptionCode exception_code, XReg tval
  description {
    Given an exception code and a *legal* non-zero value for stval,
    returns the value to be written in stval considering implementation options
  }
  body {
    if (exception_code == ExceptionCode::Breakpoint) {
      return REPORT_VA_IN_STVAL_ON_BREAKPOINT ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadAddressMisaligned) {
      return REPORT_VA_IN_STVAL_ON_LOAD_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoAddressMisaligned) {
      return REPORT_VA_IN_STVAL_ON_STORE_AMO_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionAddressMisaligned) {
      return REPORT_VA_IN_STVAL_ON_INSTRUCTION_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadAccessFault) {
      return REPORT_VA_IN_STVAL_ON_LOAD_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoAccessFault) {
      return REPORT_VA_IN_STVAL_ON_STORE_AMO_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionAccessFault) {
      return REPORT_VA_IN_STVAL_ON_INSTRUCTION_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadPageFault) {
      return REPORT_VA_IN_STVAL_ON_LOAD_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoPageFault) {
      return REPORT_VA_IN_STVAL_ON_STORE_AMO_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionPageFault) {
      return REPORT_VA_IN_STVAL_ON_INSTRUCTION_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::IllegalInstruction) {
      return REPORT_ENCODING_IN_STVAL_ON_ILLEGAL_INSTRUCTION ? tval : 0;
    # } else if (exception_code == ExceptionCode::SoftwareCheck) {
      # return REPORT_CAUSE_IN_STVAL_ON_SOFTWARE_CHECK ? tval : 0;
    } else {
      return 0;
    }
  }
}


function vstval_for {
  returns XReg
  arguments ExceptionCode exception_code, XReg tval
  description {
    Given an exception code and a *legal* non-zero value for vstval,
    returns the value to be written in vstval considering implementation options
  }
  body {
    if (exception_code == ExceptionCode::Breakpoint) {
      return REPORT_VA_IN_VSTVAL_ON_BREAKPOINT ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadAddressMisaligned) {
      return REPORT_VA_IN_VSTVAL_ON_LOAD_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoAddressMisaligned) {
      return REPORT_VA_IN_VSTVAL_ON_STORE_AMO_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionAddressMisaligned) {
      return REPORT_VA_IN_VSTVAL_ON_INSTRUCTION_MISALIGNED ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadAccessFault) {
      return REPORT_VA_IN_VSTVAL_ON_LOAD_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoAccessFault) {
      return REPORT_VA_IN_VSTVAL_ON_STORE_AMO_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionAccessFault) {
      return REPORT_VA_IN_VSTVAL_ON_INSTRUCTION_ACCESS_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::LoadPageFault) {
      return REPORT_VA_IN_VSTVAL_ON_LOAD_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::StoreAmoPageFault) {
      return REPORT_VA_IN_VSTVAL_ON_STORE_AMO_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::InstructionPageFault) {
      return REPORT_VA_IN_VSTVAL_ON_INSTRUCTION_PAGE_FAULT ? tval : 0;
    } else if (exception_code == ExceptionCode::IllegalInstruction) {
      return REPORT_ENCODING_IN_VSTVAL_ON_ILLEGAL_INSTRUCTION ? tval : 0;
    # } else if (exception_code == ExceptionCode::SoftwareCheck) {
      # return REPORT_CAUSE_IN_VSTVAL_ON_SOFTWARE_CHECK ? tval : 0;
    } else {
      return 0;
    }
  }
}

function raise {
  arguments ExceptionCode exception_code, XReg tval
  description {
    Raise synchronous exception number `exception_code`.
  }
  body {
    PrivilegeMode handling_mode = exception_handling_mode(exception_code);

    if (handling_mode == PrivilegeMode::M) {
      CSR[mepc].PC = PC;
      if (!mtval_readonly?()) {
        CSR[mtval].VALUE = mtval_for(exception_code, tval);
      }
      PC = {CSR[mtvec].BASE, 2'b00};
      CSR[mcause].INT = 1'b0;
      CSR[mcause].CAUSE = $bits(exception_code);
    } else if (implemented?(ExtensionName::S) && (handling_mode == PrivilegeMode::HS)) {
      CSR[sepc].PC = PC;
      if (!stval_readonly?()) {
        CSR[stval].VALUE = stval_for(exception_code, tval);
      }
      PC = {CSR[stvec].BASE, 2'b00};
      CSR[scause].INT = 1'b0;
      CSR[scause].CODE = $bits(exception_code);
    } else if (implemented?(ExtensionName::H) && (handling_mode == PrivilegeMode::VS)) {
      CSR[vsepc].PC = PC;
      if (!vstval_readonly?()) {
        CSR[vstval].VALUE = vstval_for(exception_code, tval);
      }
      PC = {CSR[vstvec].BASE, 2'b00};
      CSR[vscause].INT = 1'b0;
      CSR[vscause].CODE = $bits(exception_code);
    }

    # abort the current instruction, and start to refetch from PC
    set_mode(handling_mode);
    abort_current_instruction();
  }
}

function ialign {
  returns Bits<6>
  description {
    Returns IALIGN, the smallest instruction encoding size, in bits.
  }
  body {
    if (implemented?(ExtensionName::C) && (CSR[misa].C == 0x1)) {
      return 16;
    } else {
      return 32;
    }
  }
}

function jump {
  arguments XReg target_addr
  description {
    Jump to virtual address `target_addr`.

    If target address is misaligned, raise a `MisalignedAddress` exception.
  }
  body {
    # raise a misaligned exception if address is not aligned to IALIGN
    if ((ialign() == 16) && ((target_addr & 0x1) != 0)) {
      # the target PC is not halfword-aligned
      raise(ExceptionCode::InstructionAddressMisaligned, target_addr);
    } else if ((target_addr & 0x3) != 0) {
      # the target PC is not word-aligned
      raise(ExceptionCode::InstructionAddressMisaligned, target_addr);
    }

    PC = target_addr;
  }
}

function jump_halfword {
  arguments XReg target_hw_addr
  description {
    Jump to virtual halfword address `target_hw_addr`.

    If target address is misaligned, raise a `MisalignedAddress` exception.
  }
  body {
    # ensure that the target address is really a halfword address
    assert((target_hw_addr & 0x1) == 0x0, "Expected halfword-aligned address in jump_halfword");

    if (ialign() != 16) {
      if ((target_hw_addr & 0x3) != 0) {
        # the target PC is not word-aligned
        raise(ExceptionCode::InstructionAddressMisaligned, target_hw_addr);
      }
    }

    PC = target_hw_addr;
  }
}

function valid_interrupt_code? {
  returns Boolean
  arguments XReg code
  description {
    Returns true if _code_ is a legal interrupt number.
  }
  body {
    if (code > ((1 << $enum_element_size(InterruptCode)) - 1)) {
      # code is too large
      return false;
    }
    if (ary_includes?<$enum_size(InterruptCode), $enum_element_size(InterruptCode)>($enum_to_a(InterruptCode), code)) {
      return true;
    } else {
      return false;
    }
  }
}

function valid_exception_code? {
  returns Boolean
  arguments XReg code
  description {
    Returns true if _code_ is a legal exception number.
  }
  body {
    if (code > ((1 << $enum_element_size(ExceptionCode)) - 1)) {
      # code is too large
      return false;
    }
    if (ary_includes?<$enum_size(InterruptCode), $enum_element_size(InterruptCode)>($enum_to_a(InterruptCode), code)) {
      return true;
    } else {
      return false;
    }
  }
}

function xlen {
  returns Bits<8>
  description {
    Returns the effective XLEN for the current privilege mode.
  }
  body {
    if (XLEN == 32) {
      return 32;
    }

    if (mode() == PrivilegeMode::M) {
      if (CSR[misa].MXL == $bits(XRegWidth::XLEN32)) {
        return 32;
      } else if (CSR[misa].MXL == $bits(XRegWidth::XLEN64)) {
        return 64;
      }
    } else if (implemented?(ExtensionName::S) && mode() == PrivilegeMode::S) {
      if (CSR[mstatus].SXL == $bits(XRegWidth::XLEN32)) {
        return 32;
      } else if (CSR[mstatus].SXL == $bits(XRegWidth::XLEN64)) {
        return 64;
      }
    } else if (implemented?(ExtensionName::U) && mode() == PrivilegeMode::U) {
      if (CSR[mstatus].UXL == $bits(XRegWidth::XLEN32)) {
        return 32;
      } else if (CSR[mstatus].UXL == $bits(XRegWidth::XLEN64)) {
        return 64;
      }
    } else if (implemented?(ExtensionName::H) && mode() == PrivilegeMode::VS) {
      if (CSR[hstatus].VSXL == $bits(XRegWidth::XLEN32)) {
        return 32;
      } else if (CSR[hstatus].VSXL == $bits(XRegWidth::XLEN64)) {
        return 64;
      }
    } else if (implemented?(ExtensionName::H) && mode() == PrivilegeMode::VU) {
      if (CSR[vsstatus].UXL == $bits(XRegWidth::XLEN32)) {
        return 32;
      } else if (CSR[vsstatus].UXL == $bits(XRegWidth::XLEN64)) {
        return 64;
      }
    }
  }
}

function virtual_mode? {
  returns Boolean
  description {
    Returns True if the current mode is virtual (VS or VU).
  }
  body {
    return (mode() == PrivilegeMode::VS) || (mode() == PrivilegeMode::VU);
  }
}

function mask_eaddr {
  returns XReg
  arguments XReg eaddr
  description {
    Mask upper N bits of an effective address if pointer masking is enabled
  }
  body {
    #if (implemented?(ExtensionName::Zjpm)) {
    #  if (mode() == PrivilegeMode::M) {
    #    if (CSR[mpm].menable) {
    #      # ignore upper mbits of effective address
    #      return sext(xlen() - CSR[mpm].mbits, eaddr);
    #    }
    #  } else if (mode() == PrivilegeMode::S) { # also applies to HS mode
    #    if (CSR[spm].senable) {
    #      # ignore upper sbits of effective address
    #      return sext(xlen() - CSR[spm].sbits, eaddr);
    #    }
    #  } else if (mode() == PrivilegeMode::U || mode() == PrivilegeMode::VU) {
    #    if (CSR[upm].uenable) {
    #      # ignore upper ubits of effective address
    #      return sext(xlen() - CSR[upm].ubits, eaddr);
    #    }
    #  }
    #}

    # by default, eaddr == vaddr
    return eaddr;
  }
}


bitfield (8) PmpCfg {
  L 7
  Rsvd 6-5
  A 4-3
  X 2
  W 1
  R 0
}

enum PmpCfg_A {
  OFF 0
  TOR 1
  NA4 2
  NAPOT 3
}

enum PmpMatchResult {
  NoMatch 0
  FullMatch 1
  PartialMatch 2
}

function pmp_match_64 {
  template U32 access_size
  returns PmpMatchResult, PmpCfg
  arguments Bits<PHYS_ADDR_WIDTH> paddr
  description {
    Given a physical address, see if any PMP entry matches.
    
    If there is a complete match, return the PmpCfg that guards the region.
    If there is no match or a partial match, report that result.
  }
  body {
    Bits<12> pmpcfg0_addr = 0x3a0;
    Bits<12> pmpaddr0_addr = 0x3b0;

    for (U32 i=0; i<NUM_PMP_ENTRIES; i++) {
      # get the registers for this PMP entry
      Bits<12> pmpcfg_idx = pmpcfg0_addr + (i/8)*2;
      Bits<6> shamt = (i % 8)*8;
      PmpCfg cfg = ($bits(CSR[pmpcfg0_addr]) >> shamt)[7:0];
      Bits<12> pmpaddr_idx = pmpaddr0_addr + i;

      # set up the default range limits, which will result in NoMatch when
      # compared to the access
      Bits<PHYS_ADDR_WIDTH> range_hi = 0;
      Bits<PHYS_ADDR_WIDTH> range_lo = 0;

      if (cfg.A == $bits(PmpCfg_A::TOR)) {
        if (i == 0) {
          # when entry zero is TOR, zero is the lower address bound
          range_lo = 0;
        } else {
          # otherwise, it's the address in the next lowest pmpaddr register
          range_lo = (CSR[pmpaddr_idx - 1] << 2)[PHYS_ADDR_WIDTH-1:0];
        }
        range_hi = (CSR[pmpaddr_idx] << 2)[PHYS_ADDR_WIDTH-1:0];

      } else if (cfg.A == $bits(PmpCfg_A::NAPOT)) {
        # Example pmpaddr: 0b00010101111
        #                          ^--- last 0 dictates region size & alignment
        # pmpaddr + 1:     0b00010110000
        # mask:            0b00000011111
        # ~mask:           0b11111100000
        # len = mask + 1:  0b00000100000
        Bits<PHYS_ADDR_WIDTH-2> pmpaddr_value = CSR[pmpaddr_idx].sw_read()[PHYS_ADDR_WIDTH-3:0];
        Bits<PHYS_ADDR_WIDTH-2> mask = pmpaddr_value ^ (pmpaddr_value + 1);
        range_lo = (pmpaddr_value & ~mask) << 2;
        Bits<PHYS_ADDR_WIDTH-2> len = mask + 1;
        range_hi = ((pmpaddr_value & ~mask) + len) << 2;

      } else if (cfg.A == $bits(PmpCfg_A::NA4)) {
        range_lo = (CSR[pmpaddr_idx] << 2)[PHYS_ADDR_WIDTH-1:0];
        range_hi = range_lo + 4;
      }

      if ((paddr >= range_lo) && ((paddr + (access_size/8)) < range_hi)) {
        # full match
        return PmpMatchResult::FullMatch, cfg;
      } else if (!(((paddr + (access_size/8) - 1) < range_lo) || (paddr >= range_hi))) {
        # this is a partial match. By definition, the access must fail, regardless
        # of the pmp cfg settings
        return PmpMatchResult::PartialMatch, -;
      }
    }
    # fall-through: there was no match
    return PmpMatchResult::NoMatch, -;
  }
}

function pmp_match_32 {
  template U32 access_size
  returns PmpMatchResult, PmpCfg
  arguments Bits<PHYS_ADDR_WIDTH> paddr
  description {
    Given a physical address, see if any PMP entry matches.
    
    If there is a complete match, return the PmpCfg that guards the region.
    If there is no match or a partial match, report that result.
  }
  body {
    Bits<12> pmpcfg0_addr = 0x3a0;
    Bits<12> pmpaddr0_addr = 0x3b0;

    for (U32 i=0; i<NUM_PMP_ENTRIES; i++) {
      # get the registers for this PMP entry
      Bits<12> pmpcfg_idx = pmpcfg0_addr + (i/4);
      Bits<6> shamt = (i % 4)*8;
      PmpCfg cfg = ($bits(CSR[pmpcfg0_addr]) >> shamt)[7:0];
      Bits<12> pmpaddr_idx = pmpaddr0_addr + i;

      # set up the default range limits, which will result in NoMatch when
      # compared to the access
      Bits<PHYS_ADDR_WIDTH> range_hi = 0;
      Bits<PHYS_ADDR_WIDTH> range_lo = 0;

      if (cfg.A == $bits(PmpCfg_A::TOR)) {
        if (i == 0) {
          # when entry zero is TOR, zero is the lower address bound
          range_lo = 0;
        } else {
          # otherwise, it's the address in the next lowest pmpaddr register
          range_lo = (CSR[pmpaddr_idx - 1] << 2)[PHYS_ADDR_WIDTH-1:0];
        }
        range_hi = (CSR[pmpaddr_idx] << 2)[PHYS_ADDR_WIDTH-1:0];

      } else if (cfg.A == $bits(PmpCfg_A::NAPOT)) {
        # Example pmpaddr: 0b00010101111
        #                          ^--- last 0 dictates region size & alignment
        # pmpaddr + 1:     0b00010110000
        # mask:            0b00000011111
        # ~mask:           0b11111100000
        # len = mask + 1:  0b00000100000
        Bits<PHYS_ADDR_WIDTH-2> pmpaddr_value = CSR[pmpaddr_idx].sw_read()[PHYS_ADDR_WIDTH-3:0];
        Bits<PHYS_ADDR_WIDTH-2> mask = pmpaddr_value ^ (pmpaddr_value + 1);
        range_lo = (pmpaddr_value & ~mask) << 2;
        Bits<PHYS_ADDR_WIDTH-2> len = mask + 1;
        range_hi = ((pmpaddr_value & ~mask) + len) << 2;

      } else if (cfg.A == $bits(PmpCfg_A::NA4)) {
        range_lo = ($bits(CSR[pmpaddr_idx]) << 2)[PHYS_ADDR_WIDTH-1:0];
        range_hi = range_lo + 4;
      }

      if ((paddr >= range_lo) && ((paddr + (access_size/8)) < range_hi)) {
        # full match
        return PmpMatchResult::FullMatch, cfg;
      } else if (!(((paddr + (access_size/8) - 1) < range_lo) || (paddr >= range_hi))) {
        # this is a partial match. By definition, the access must fail, regardless
        # of the pmp cfg settings
        return PmpMatchResult::PartialMatch, -;
      }
    }
    # fall-through: there was no match
    return PmpMatchResult::NoMatch, -;
  }
}

function pmp_match {
  template U32 access_size
  returns PmpMatchResult, PmpCfg
  arguments Bits<PHYS_ADDR_WIDTH> paddr
  description {
    Given a physical address, see if any PMP entry matches.
    
    If there is a complete match, return the PmpCfg that guards the region.
    If there is no match or a partial match, report that result.
  }
  body {
    if (XLEN == 64) {
      return pmp_match_64<access_size>(paddr);
    } else {
      return pmp_match_32<access_size>(paddr);
    }
  }
}

function mpv {
  returns Bits<1>
  description {
    Returns the current value of CSR[mstatus].MPV (when MXLEN == 64) of CSR[mstatush].MPV (when MXLEN == 32)
  }
  body {
    if (implemented?(ExtensionName::H)) {
      return (XLEN == 32) ? CSR[mstatush].MPV : CSR[mstatus].MPV;
    } else {
      assert(false, "TODO");
    }
  }
}


function effective_ldst_mode {
  returns PrivilegeMode
  description {
    Returns the effective privilege mode for normal explicit loads and stores, taking into account
    the current actual privilege mode and modifications from `mstatus.MPRV`.
  }
  body {
    # when the mode is M, loads and stores can be executed as if they were done from any other mode
    # with the use of mstatus.MPRV
    if (mode() == PrivilegeMode::M) {
      if (implemented?(ExtensionName::U) && CSR[mstatus].MPRV == 1) {
        if (CSR[mstatus].MPP == 0b00) {
          if (implemented?(ExtensionName::H) && mpv() == 0b1) {
            return PrivilegeMode::VU;
          } else {
            return PrivilegeMode::U;
          }
        } else if (implemented?(ExtensionName::S) && CSR[mstatus].MPP == 0b01) {
          if (implemented?(ExtensionName::H) && mpv() == 0b1) {
            return PrivilegeMode::VS;
          } else {
            return PrivilegeMode::S;
          }
        }
      }
    }

    # no modifiers were found, return actual mode
    return mode();
  }
}


function pmp_check {
  template U32 access_size
  returns Boolean
  arguments Bits<PHYS_ADDR_WIDTH> paddr, MemoryOperation type
  description {
    Given a physical address and operation type, return whether or not the access is allowed by PMP.
  }
  body {
    PrivilegeMode mode = effective_ldst_mode();
    PmpMatchResult match_result;
    PmpCfg cfg;

    (match_result, cfg) = pmp_match<access_size>(paddr);

    if (match_result == PmpMatchResult::FullMatch) {
      if (mode == PrivilegeMode::M && (cfg.L == 0)) {
        # when the region is not locked, all M-mode access pass
        return true;
      }

      # this is either an HS, VS, VU, or U mode access, or an M mode access with cfg.L set
      # the RWX settings in cfg apply
      if (type == MemoryOperation::Write && (cfg.W == 0)) {
        return false;
      } else if (type == MemoryOperation::Read && (cfg.R == 0)) {
        return false;
      } else if (type == MemoryOperation::Fetch && (cfg.X == 0)) {
        return false;
      }
    } else if (match_result == PmpMatchResult::NoMatch) {
      # with no matched, M-mode passes and everything else fails
      if (mode == PrivilegeMode::M) {
        return true;
      } else {
        return false;
      }
    } else {
      assert(match_result == PmpMatchResult::PartialMatch, "PMP matching logic error");

      # by defintion, any partial match fails the access, regardless of the config settings
      return false;
    }

    # fall-through passes
    return true;
  }
}

function access_check {
  template U32 access_size
  arguments Bits<PHYS_ADDR_WIDTH> paddr, XReg vaddr, MemoryOperation type, ExceptionCode fault_type
  description {
    Checks if the physical address paddr is able to access memory, and raises
    the appropriate exception if not.
  }
  body {
    # check if this is a valid physical address
    if (paddr > ((1 << PHYS_ADDR_WIDTH) - access_size)) {
      raise(fault_type, vaddr);
    }

    # check PMP
    if (!pmp_check<access_size>(paddr[PHYS_ADDR_WIDTH-1:0], type)) {
      raise(fault_type, vaddr);
    }
  }
}

function base32? {
  returns Boolean
  description {
    return True iff current effective XLEN == 32
  }
  body {
    XRegWidth xlen32 = XRegWidth::XLEN32;
    if (mode() == PrivilegeMode::M) {
      return CSR[misa].MXL == $bits(xlen32);
    } else if (implemented?(ExtensionName::S) && mode() == PrivilegeMode::S) {
      return CSR[mstatus].SXL == $bits(xlen32);
    } else if (implemented?(ExtensionName::U) && mode() == PrivilegeMode::U) {
      return CSR[mstatus].UXL == $bits(xlen32);
    } else if (implemented?(ExtensionName::H) && mode() == PrivilegeMode::VS) {
      return CSR[hstatus].VSXL == $bits(xlen32);
    } else {
      assert(implemented?(ExtensionName::H) && mode() == PrivilegeMode::VU, "Unexpected mode");
      return CSR[vsstatus].UXL == $bits(xlen32);
    }
  }
}

function base64? {
  returns Boolean
  description {
    return True iff current effective XLEN == 64
  }
  body {
    return xlen() == 64;
  }
}

function translate_load_gstage {
  returns Bits<PHYS_ADDR_WIDTH>
  arguments XReg gpaddr
  description {
    Tranlate guest physical address into host physical address.
  }
  body {
    XReg ppn;
    if (base32?()) {
      return 0;
    } else {
      return 0;
    }
  }
}

function translate_load_sv32 {
  returns Bits<34>
  arguments XReg vaddr, XReg satp
  description {
    Translate virtual address using Sv32
  }
  body {
    return 0;
  }
}

function current_translation_mode {
  returns SatpMode
  description {
    Returns the current translation mode for a load or store
    given the machine state (e.g., value of `satp` csr).
  }
  body {
    PrivilegeMode effective_mode = effective_ldst_mode();

    SatpMode translation_mode;

    if (effective_mode == PrivilegeMode::M) {
      return SatpMode::Bare;
    } else if (CSR[misa].S == 1'b1) {
      # if (implemented?(ExtensionName::H) && virtual_mode?()) {
        # return CSR[vsatp].MODE;
      # } else {
        return CSR[satp].MODE;
      # }
    } else {
      # there is no S mode, and so there is no translation in what must be U-mode
      return SatpMode::Bare;
    }
  }
}

function page_walk {
  template
    U32 VA_SIZE,     # virtual address size  (Sv32 = 32, Sv39 = 39, Sv48 = 48, Sv57 = 57)
    U32 PA_SIZE,     # physical address size (Sv32 = 34, Sv39 = 56, Sv48 = 56, Sv57 = 56)
    U32 PTESIZE,     # length, in bits, of a Page Table Entry (Sv32 = 4, others = 8)
    U32 LEVELS       # levels in the page tabl (Sv32 = 2, Sv39 = 3, Sv48 = 4, Sv57 = 5)
  returns Bits<PA_SIZE> # the translated address
  arguments
    Bits<XLEN> vaddr,  # the virtual address to translate
    MemoryOperation op # the operation type
  description {
    Translate virtual address through a page walk.

    May raise a Page Fault if an error involving the page table structure occurs along the walk.

    Implicit reads of the page table are accessed check, and may raise Access Faults.
    Implicit writes (updates of A/D) are also accessed checked, and may raise Access Faults

    The translated address _is not_ accessed checked.

    Returns the translated physical address.
  }
  body {
    Bits<PA_SIZE> ppn;

    # the VPN size is 10 bits in Sv32, and 9 bits in all others
    U32 VPN_SIZE = (LEVELS == 2) ? 10 : 9;

    # if there is an exception, set up the correct type
    ExceptionCode access_fault_code =
      op == MemoryOperation::Read ?
        ExceptionCode::LoadAccessFault :
        ( op == MemoryOperation::Fetch ?
            ExceptionCode::InstructionAccessFault :
            ExceptionCode::StoreAmoAccessFault );

    ExceptionCode page_fault_code =
      op == MemoryOperation::Read ?
        ExceptionCode::LoadPageFault :
        ( op == MemoryOperation::Fetch ?
            ExceptionCode::InstructionPageFault :
            ExceptionCode::StoreAmoPageFault );

#    if (virtual_mode?()) {
#      ppn = CSR[vsatp].ppn;
#    } else {
      ppn = CSR[satp].PPN;
#    }

    if (vaddr[xlen()-1:VA_SIZE] != {xlen()-VA_SIZE{vaddr[VA_SIZE - 1]}}) {
      # non-canonical virtual address raises a page fault
      # note that if pointer masking is enabled,
      # vaddr has already been transformed before reaching here
      raise (page_fault_code, vaddr);
    }

    for (U32 i = (LEVELS - 1); i >= 0; i--) {
      U32 vpn = (vaddr >> (12 + VPN_SIZE*i)) & ((1 << VPN_SIZE) - 1);

      Bits<PA_SIZE> pte_addr = (ppn << 12) + (vpn * (PTESIZE/8));

      # perform access check on the physical address of pte before it's used
      access_check<PTESIZE>(pte_addr, vaddr, MemoryOperation::Read, access_fault_code);

      if (!pma_applies?(PmaAttribute::HardwarePageTableRead, pte_addr, PTESIZE)) {
        raise(access_fault_code, vaddr);
      }

      Bits<PTESIZE> pte = read_physical_memory<PTESIZE>(pte_addr);
      PteFlags pte_flags = pte[9:0];

      # check if any reserved bits are set
      # Sv39 has no reserved bits, and Sv39/48/57 all have reserved bits at 60:54
      if ((VA_SIZE != 32) && (pte[60:54] != 0)) {
        raise(page_fault_code, vaddr);
      }

      if (pte_flags.V == 0 ||                    # invalid entry
         (pte_flags.R == 0 && pte_flags.W == 1)) { # write permission must also have read permission
#          || (!implemented?(ExtensionName::Svnapot) && pte.N != 0)
#          || (!implemented?(ExtensionName::Svpbmt) && pte.PBMT != 0)) {
        # found invalid PTE
        raise (page_fault_code, vaddr);
      } else if (pte_flags.R == 1 || pte_flags.X == 1) {
        # found a leaf PTE

        # see if there is permission to perform the access
        if (op == MemoryOperation::Read || op == MemoryOperation::ReadModifyWrite) {
          if ((CSR[mstatus].MXR == 0 && pte_flags.R == 0)
              || (CSR[mstatus].MXR == 1 && pte_flags.X == 0 && pte_flags.R == 0)) {
            # no read permission
            raise (page_fault_code, vaddr);
          }
          if ((mode() == PrivilegeMode::U && pte_flags.U == 0)
            || (mode() == PrivilegeMode::M && CSR[mstatus].MPRV == 1 && CSR[mstatus].MPP == $bits(PrivilegeMode::HS) && pte_flags.U == 1 && CSR[mstatus].SUM == 0)
            || (mode() == PrivilegeMode::S && pte_flags.U == 1 && CSR[mstatus].SUM == 0)) {
            # supervisor cannot read U unless mstatus.SUM = 1
            raise (page_fault_code, vaddr);
          }
        }
        if (((op == MemoryOperation::Write) || (op == MemoryOperation::ReadModifyWrite))
             && (pte_flags.W == 0)) {
          # no write permission
          raise (page_fault_code, vaddr);
        } else if ((op == MemoryOperation::Fetch) && (pte_flags.X == 0)) {
          # no execute permission
          raise (page_fault_code, vaddr);
        }

        if (pte_flags.U == 0) {
          # supervisor page
          if (mode() == PrivilegeMode::U) {
            # user access to supervisor page is never allowed
            raise (page_fault_code, vaddr);
          }
        } else {
          # user page
          if (mode() == PrivilegeMode::S || (mode() == PrivilegeMode::M && CSR[mstatus].MPRV == 1 && CSR[mstatus].MPP == $bits(PrivilegeMode::S))) {
            # effective S-mode access
            if (op == MemoryOperation::Read || op == MemoryOperation::ReadModifyWrite) {
              if (CSR[mstatus].SUM == 0) {
                # supervisor can only read user pages when mstatus.SUM == 1
                raise (page_fault_code, vaddr);
              }
            } else {
              # supervisor can never write or execute a user page
              raise (page_fault_code, vaddr);
            }
          }
        }

        # ensure remaining PPN bits are zero, otherwise there is a misaligned super page
        raise (page_fault_code, vaddr) if ((i > 0) && (pte[(i-1)*VPN_SIZE:10] != 0));

        # check access and dirty bits
        if ((pte_flags.A == 0)         # access is clear
             || ((pte_flags.D == 0)    # or dirty is clear and this is a (read-modify-)write
                 && ((op == MemoryOperation::Write)
                     || (op == MemoryOperation::ReadModifyWrite)))) {

          # check for hardware update of A/D bits
          if (CSR[menvcfg].ADUE == 1'b1) {
            # Svadu requires page tables to be located in memory with hardware page-table write access
            # and RsrvEventual PMA
            if (!pma_applies?(PmaAttribute::RsrvEventual, pte_addr, PTESIZE)) {
              raise (access_fault_code, vaddr);
            }
            if (!pma_applies?(PmaAttribute::HardwarePageTableWrite, pte_addr, PTESIZE)) {
              raise (access_fault_code, vaddr);
            }

            access_check<PTESIZE>(pte_addr, vaddr, MemoryOperation::Write, access_fault_code);

            Boolean success;
            Bits<PTESIZE> updated_pte;
            if (pte_flags.D == 0 && op == MemoryOperation::Write) {
              # try to set both A and D bits
              updated_pte = pte | 0b11000000;
            } else {
              # try to set the A bit
              updated_pte = pte | 0b01000000;
            }

            if (PTESIZE == 32) {
              success = atomic_check_then_write_32(pte_addr, pte, updated_pte);
            } else if (PTESIZE == 64) {
              success = atomic_check_then_write_64(pte_addr, pte, updated_pte);
            } else {
              assert(false, "Unexpected PTESIZE");
            }


            if (!success) {
              # the PTE changed between the read during the walk and the attempted atomic update
              # roll back, and try this level again
              i = i + 1;
            } else {
              # successful translation and update
              return {(pte[PA_SIZE-3:(i*VPN_SIZE) + 10] << 2), vaddr[11:0]};
            }
          }

          if ((CSR[menvcfg].ADUE == 1'b0) && implemented?(ExtensionName::Svade)) {
            raise(page_fault_code, vaddr);
          }
        }

        # translation succeeded
        return {(pte[PA_SIZE-3:(i*VPN_SIZE) + 10] << 2), vaddr[11:0]};
      } else {
        # found a pointer to the next level

        if (i == 0) {
          # a pointer can't exist on the last level
          raise (page_fault_code, vaddr);
        }

        if (pte_flags.D == 1 || pte_flags.A == 1 || pte_flags.U == 1) {
          # D, A, and U are reserved in non-leaf PTEs
          raise (page_fault_code, vaddr);
        }

        # fall through to next level
        ppn = pte[PA_SIZE-3:10] << 12;
      }
    }
  }
}

function translate {
  returns Bits<PHYS_ADDR_WIDTH>
  arguments XReg vaddr, MemoryOperation op
  description {
    Translate a virtual address for a load, returning a physical address.
    May raise a Page Fault or Access Fault.

    The final physical address is *not* access checked (for PMP, PMA, etc., violations).
  }
  body {
    SatpMode translation_mode = current_translation_mode();

    XReg paddr;

    if (implemented?(ExtensionName::H) && virtual_mode?()) {
      return 0;
#      if (translation_mode == SatpMode::Bare) {
#        # bare == no translation
#        return translate_gstage(vaddr, op);
#
#      } else if (implemented?(Sv32) && translation_mode == SatpMode::Sv32) {
#        XReg gpaddr = translate_sv32(vaddr, CSR[satp], op);
#        return translate_gstage(gpaddr, op);
#
#      } else if (implemented?(Sv39) && translation_mode == SatpMode::Sv39) {
#        XReg gpaddr = translate_sv39(vaddr, CSR[satp], op);
#        return translate_gstage(gpaddr, op);
#
#      } else if (implemented?(Sv48) && translation_mode == SatpMode::Sv48) {
#        XReg gpaddr = translation_sv48(vaddr, CSR[satp], op);
#        return translate_gstage(gpaddr, op);
#
#      } else if (implemented?(Sv57) && translation_mode == SatpMode::Sv57) {
#        XReg gpaddr = translate_sv57(vaddr, CSR[satp], op);
#        return translate_gstage(gpaddr, op);
#      }
    } else {
      if (translation_mode == SatpMode::Bare) {
        # bare == no translation
        paddr = vaddr;
      } else if (implemented?(ExtensionName::Sv32) && translation_mode == SatpMode::Sv32) {
        # Sv32 page table walk
        paddr = page_walk<32, 34, 32, 2>(vaddr, op);
      } else if (implemented?(ExtensionName::Sv39) && translation_mode == SatpMode::Sv39) {
        # Sv39 page table walk
        paddr = page_walk<39, 56, 64, 3>(vaddr, op);
      } else if (implemented?(ExtensionName::Sv48) && translation_mode == SatpMode::Sv48) {
        # Sv48 page table walk
        paddr = page_walk<48, 56, 64, 4>(vaddr, op);
      } else if (implemented?(ExtensionName::Sv57) && translation_mode == SatpMode::Sv57) {
        # Sv57 page table walk
        paddr = page_walk<57, 56, 64, 5>(vaddr, op);
      }
    }
    return paddr;
  }
}

function canonical_vaddr? {
  returns Boolean
  arguments XReg vaddr
  description {
    Returns whether or not _vaddr_ is a valid (_i.e._, canonical) virtual address.

    If pointer masking (S**pm) is enabled, then vaddr will be masked before checking
    the canonical address.
  }
  body {
    if (CSR[misa].S == 1'b0) {
      # there is no translation, any address is canonical
      return true;
    }

    # canonical depends on the virtual address size in the current translation mode
    SatpMode satp_mode;
    if (virtual_mode?()) {
      satp_mode = CSR[vsatp];
    } else {
      satp_mode = CSR[satp];
    }

    # calculate the effective address after pointer masking
    XReg eaddr = mask_eaddr(vaddr);

    if (satp_mode == SatpMode::Bare) {
      return true;
    } else if (satp_mode == SatpMode::Sv32) {
      # Sv32 uses all 32 bits of the VA
      return true; 
    } else if (satp_mode == SatpMode::Sv39) {
      return eaddr[63:39] == {25{eaddr[38]}};
    } else if (satp_mode == SatpMode::Sv48) {
      return eaddr[63:48] == {16{eaddr[47]}};
    } else if (satp_mode == SatpMode::Sv57) {
      return eaddr[63:57] == {6{eaddr[56]}};
    }
  }
}

function misaligned_is_atomic? {
  template U32 N
  returns Boolean
  arguments Bits<PHYS_ADDR_WIDTH> physical_address
  description {
    Returns true if an access starting at +physical_address+ that is +N+ bits long is atomic.

    This function takes into account any Atomicity Granule PMAs, so *it should not be used
    for load-reserved/store-conditional*, since those PMAs do not apply to those accesses.
  }
  body {
    # if the hart doesn't support Misligned Atomicity Granules,
    # then this misligned access is not atomic
    return false if MAX_MISALIGNED_ATOMICITY_GRANULE_SIZE == 0;

    if (pma_applies?(PmaAttribute::MAG16, physical_address, N) &&
        in_naturally_aligned_region?<M, 128>(physical_address_address, N)) {
      return true;
    } else if (pma_applies?(PmaAttribute::MAG8, physical_address, N) &&
                in_naturally_aligned_region?<XLEN, 64>(physical_address, N)) {
      return true;
    } else if (pma_applies?(PmaAttribute::MAG4, physical_address, N) &&
                in_naturally_aligned_region?<XLEN, 32>(physical_address, N)) {
      return true;
    } else if (pma_applies?(PmaAttribute::MAG2, physical_address, N) &&
                in_naturally_aligned_region?<XLEN, 16>(physical_address, N)) {
      return true;
    } else {
      # not saved by a Misaligned Atomicity Granule
      return false;
    }
  }
}

function read_memory_aligned {
  template U32 len
  returns Bits<len>
  arguments XReg virtual_address
  description {
    Read from virtual memory using a known aligned address.
  }
  body {
    XReg physical_address;

    physical_address = (CSR[misa].S == 1)
      ? translate(virtual_address, MemoryOperation::Read)
      : virtual_address;

    # may raise an exception
    access_check<len>(physical_address, virtual_address, MemoryOperation::Read, ExceptionCode::LoadAccessFault);

    return read_physical_memory<len>(physical_address);
  }
}

function read_memory {
  template U32 len
  returns Bits<len>
  arguments XReg virtual_address
  description {
    Read from virtual memory.

  }
  body {
    Boolean aligned = is_naturally_aligned<XLEN, len>(virtual_address);
    XReg physical_address;

    if (aligned) {
      return read_memory_aligned<len>(virtual_address);
    }
    
    # access isn't naturally aligned, but it still might be atomic if this hart supports
    # Misliagned Atomicity Granules. We won't know that, though, until after translation since PMAs
    # apply to physical addresses
    if (MAX_MISALIGNED_ATOMICITY_GRANULE_SIZE > 0) {
      # sanity check that the implementation isn't expecting a Misaligned exception
      # before an access/page fault exception (that would be an invalid config)
      assert(MISALIGNED_LDST_EXCEPTION_PRIORITY == "low");

      physical_address = (CSR[misa].S == 1)
        ? translate(virtual_address, MemoryOperation::Read)
        : virtual_address;

      if (misaligned_is_atomic?<len>(physical_address)) {
        access_check<len>(physical_address, virtual_address, MemoryOperation::Read, ExceptionCode::LoadAccessFault);
        return read_physical_memory<len>(physical_address);
      }
    }

    # at this point, we have a misligned access
    if (!MISALIGNED_LDST) {
      # misaligned is not supported, we'll raise either a Misaligned exception or
      # a page/access fault exception, depending on the configuration
      if (MISALIGNED_LDST_EXCEPTION_PRIORITY == "low") {
        # do translation to trigger any access/page faults before raising misaligned
        physical_address = (CSR[misa].S == 1)
          ? translate(virtual_address, MemoryOperation::Read)
          : virtual_address;
        access_check<len>(physical_address, virtual_address, MemoryOperation::Read, ExceptionCode::LoadAccessFault);
      }
      raise (ExceptionCode::LoadAddressMisaligned, virtual_address);
    } else {

      # misaligned, must break into multiple reads
      if (MISALIGNED_SPLIT_STRATEGY == "by_byte") {
        Bits<len> result = 0;
        for (U32 i = 0; i <= len; i++) {
          result = result | (read_memory_aligned<8>(virtual_address + i) << (8*i));
        }
        return result;
      } else if (MISALIGNED_SPLIT_STRATEGY == "custom") {
        unpredictable("An implementation is free to break a misaligned access any way, leading to unpredictable behavior when any part of the misaligned access causes an exception");
      }
    }
  }
}

# hart-global state to track the local reservation set
Boolean    reservation_set_valid = false;
Bits<XLEN> reservation_set_address;
Bits<XLEN> reservation_set_size;
Bits<XLEN> reservation_physical_address;  # The exact address used by the LR.W/LR.D
Bits<XLEN> reservation_virtual_address;   # The exact address used by the LR.W/LR.D
Bits<XLEN> reservation_size;              # the size of the LR operation (32 or 64)

function invalidate_reservation_set {
  description {
    Invalidates any currently held reservation set.

    [NOTE]
    --
    This function may be called by the platform, independent of any actions
    occurring in the local hart, for any or no reason.

    The platorm *must* call this function if an external hart or device accesses
    part of this reservation set while reservation_set_valid could be true.
    --
  }
  body {
    reservation_set_valid = false;
  }
}

function register_reservation_set {
  arguments
    Bits<XLEN> physical_address,   # The (always aligned) physical address to reserve.
    Bits<XLEN> length              # mimimum length of the reservation. actual reservation may be larger
  description {
    Register a reservation for a physical address range that subsumes
    [physical_address, physical_address + N).
  }
  body {
    reservation_set_valid = true;
    reservation_address = physical_address;

    if (LRSC_RESERVATION_STRATEGY == "reserve naturally-aligned 64-byte region") {
      reservation_set_address = physical_address & ~XLEN'h3f;
      reservation_set_size = 64;
    } else if (LRSC_RESERVATION_STRATEGY == "reserve naturally-aligned 128-byte region") {
      reservation_set_address = physical_address & ~XLEN'h7f;
      reservation_set_size = 128;
    } else if (LRSC_RESERVATION_STRATEGY == "reserve exactly enough to cover the access") {
      reservation_set_address = physical_address;
      reservation_set_size = length;
    } else if (LRSC_RESERVATION_STRATEGY == "custom") {
      unpredictable("Implementations may set reservation sets of any size, as long as they cover the reserved accessed");
    } else {
      assert(false, "Unexpected LRSC_RESERVATION_STRATEGY");
    }
  }
}

function load_reserved {
  template U32 N                # the number of bits being loaded
  returns Bits<N>               # the value of memory at virtual_address
  arguments
    Bits<XLEN> virtual_address, # the virtual address to load
    Bits<1>    aq,              # acquire semantics? 0=no, 1=yes
    Bits<1>    rl               # release semantics? 0=no, 1=yes
  description {
    Register a reservation for virtual_address at least N bits long
    and read the value from memory.

    If aq is set, then also perform a memory model acquire.

    If rl is set, then also perform a memory model release (software is discouraged from doing so).

    This function assumes alignment checks have already occurred.
  }
  body {
    Bits<PHYS_ADDR_WIDTH> physical_address =
      (CSR[misa].S == 1)
        ? translate(virtual_address, MemoryOperation::Read)
        : virtual_address;

    if (pma_applies?(PmaAttribute::RsrvNone, physical_address, N)) {
      raise(ExceptionCode::LoadAccessFault, virtual_address);
    }

    if (aq == 1) {
      memory_model_acquire();
    }
    if (rl == 1) {
      memory_model_release();
    }

    register_reservation_set(physical_address, N);

    if (CSR[misa].S ==1 && LRSC_FAIL_ON_VA_SYNONYM) {
      # also need to remember the virtual address
      reservation_virtual_address = virtual_address;
    }

    return read_memory_aligned<N>(physical_address);
  }
}

function store_conditional {
  template U32 N                # number of bits being stored
  returns Boolean               # whether or not the store conditional succeeded
  arguments
    Bits<XLEN> virtual_address, # the virtual address to store to
    Bits<XLEN> value,           # the value to store
    Bits<1>    aq,              # acquire semantics? 0=no, 1=yes
    Bits<1>    rl               # release semantics? 0=no, 1=yes
  description {
    Atomically check the reservation set to ensure:

     * it is valid
     * it covers the region addressed by this store
     * the address setting the reservation set matches virtual address

    If the preceeding are met, perform the store and return 0.
    Otherwise, return 1.
  }
  body {
    Bits<PHYS_ADDR_WIDTH> physical_address =
      (CSR[misa].S == 1)
        ? translate(virtual_address, MemoryOperation::Write)
        : virtual_address;

    if (pma_applies?(PmaAttribute::RsrvNone, physical_address, N)) {
      raise(ExceptionCode::StoreAmoAccessFault, virtual_address);
    }

    # failed SC still looks like a store to memory protection
    access_check<N>(physical_address, virtual_address, MemoryOperation::Write, ExceptionCode::StoreAmoAccessFault);

    # acquire/release occur regardless of whether or not the SC succeeds
    if (aq == 1) {
      memory_model_acquire();
    }
    if (rl == 1) {
      memory_model_release();
    }

    if (reservation_set_valid == false) {
      return false;
    }

    if (!contains?(reservation_set_address, reservation_set_size, physical_address, N)) {
      # this access is not in the reservation set
      invalidate_reservation_set();
      return false;
    }

    if (LRSC_FAIL_ON_NON_EXACT_LRSC) {
      if (reservation_physical_address != physical_address || reservation_size != N) {
        # this access does not match the most recent LR
        invalidate_reservation_set();
        return false;
      }
    }

    if (LRSC_FAIL_ON_VA_SYNONYM) {
      if (reservation_virtual_address != virtual_address || reservation_size != N) {
        # this access does not match the most recent LR
        invalidate_reservation_set();
        return false;
      }
    }

    # success. perform the store
    write_physical_memory<N>(physical_address, value);

    return true;
  }
}

function amo {
  template U32 N                # number of bits being loaded/stored
  returns Bits<N>
  arguments
    Bits<XLEN> virtual_address, # the virtual address to load from/store to
    Bits<N> value,              # the value for the second hald of the atomic operation
    AmoOperation op,            # atomic operation to apply
    Bits<1>    aq,              # acquire semantics? 0=no, 1=yes
    Bits<1>    rl               # release semantics? 0=no, 1=yes
  description {
    Atomically read-modify-write the location at virtual_address.

    The value written to virtual_address will depend on +op+.

    If +aq+ is 1, then the amo also acts as a memory model acquire.
    If +rl+ is 1, then the amo also acts as a memory model release.
  }
  body {
    Boolean aligned = is_naturally_aligned<PHYS_ADDR_WIDTH, N>(virtual_address);

    if (!aligned && MISALIGNED_LDST_EXCEPTION_PRIORITY == "high") {
      raise(ExceptionCode::StoreAmoAddressMisaligned, virtual_address);
    }

    Bits<PHYS_ADDR_WIDTH> physical_address =
      (CSR[misa].S == 1)
        ? translate(virtual_address, MemoryOperation::ReadModifyWrite)
        : virtual_address;

    # PMA Atomicity checks
    if (pma_applies?(PmaAttribute::AmoNone, physical_address, N)) {
      raise(ExceptionCode::StoreAmoAccessFault, virtual_address);
    } else if (((op == AmoOperation::Add || op == AmoOperation::Max || op == AmoOperation::Maxu || op == AmoOperation::Min || op == AmoOperation::Minu)) &&
               !pma_applies?(PmaAttribute::AmoArithmetic, physical_address, N)) {
      raise(ExceptionCode::StoreAmoAccessFault, virtual_address);
    } else if (((op == AmoOperation::And || op == AmoOperation::Or || op == AmoOperation::Xor)) &&
                !pma_applies?(PmaAttribute::AmoLogical, physical_address, N)) {
      raise(ExceptionCode::StoreAmoAccessFault, virtual_address);
    } else {
      assert(
        pma_applies?(PmaAttribute::AmoSwap, physical_address, N) &&
        op == AmoOperation::Swap
      );
    }

    # pma alignment checks
    if (!aligned &&
         !misaligned_is_atomic?<N>(physical_address)) {
      raise (ExceptionCode::StoreAmoAddressMisaligned, virtual_address);
    }

    if (N == 32) {
      return atomic_read_modify_write_32(physical_address, value, op);
    } else {
      return atomic_read_modify_write_64(physical_address, value, op);
    }

  }
}



function write_memory_aligned {
  template U32 len
  arguments XReg virtual_address, Bits<len> value
  description {
    Write to virtual memory using a known aligned address.
  }
  body {
    XReg physical_address;

    physical_address = (CSR[misa].S == 1)
      ? translate(virtual_address, MemoryOperation::Write)
      : virtual_address;

    # may raise an exception
    access_check<len>(physical_address, virtual_address, MemoryOperation::Write, ExceptionCode::StoreAmoAccessFault);

    write_physical_memory<len>(physical_address, value);
  }
}

function write_memory {
  template U32 len
  arguments XReg virtual_address, Bits<len> value
  description {
    Write to virtual memory
  }
  body {
    Boolean aligned = is_naturally_aligned<XLEN, len>(virtual_address);
    XReg physical_address;

    if (aligned) {
      write_memory_aligned<len>(virtual_address, value);
    }
    
    # access isn't naturally aligned, but it still might be atomic if this hart supports
    # Misliagned Atomicity Granules. We won't know that, though, until after translation since PMAs
    # apply to physical addresses
    if (MAX_MISALIGNED_ATOMICITY_GRANULE_SIZE > 0) {
      # sanity check that the implementation isn't expecting a Misaligned exception
      # before an access/page fault exception (that would be an invalid config)
      assert(MISALIGNED_LDST_EXCEPTION_PRIORITY == "low");

      physical_address = (CSR[misa].S == 1)
        ? translate(virtual_address, MemoryOperation::Write)
        : virtual_address;

      if (misaligned_is_atomic?<len>(physical_address)) {
        access_check<len>(physical_address, virtual_address, MemoryOperation::Write, ExceptionCode::StoreAmoAccessFault);
        write_physical_memory<len>(physical_address, value);
      }
    }

    # at this point, we have a misligned access that must be split
    if (!MISALIGNED_LDST) {
      # misaligned is not supported, we'll raise either a Misaligned exception or
      # a page/access fault exception, depending on the configuration
      if (MISALIGNED_LDST_EXCEPTION_PRIORITY == "low") {
        # do translation to trigger any access/page faults before raising misaligned
        physical_address = (CSR[misa].S == 1)
          ? translate(virtual_address, MemoryOperation::Write)
          : virtual_address;
        access_check<len>(physical_address, virtual_address, MemoryOperation::Write, ExceptionCode::StoreAmoAccessFault);
      }
      raise (ExceptionCode::StoreAmoAddressMisaligned, virtual_address);
    } else {
      # misaligned, must break into multiple reads
      if (MISALIGNED_SPLIT_STRATEGY == "by_byte") {
        for (U32 i = 0; i <= len; i++) {
          write_memory_aligned<8>(virtual_address + i, (value >> (8*i))[7:0]);
        }
      } else if (MISALIGNED_SPLIT_STRATEGY == "custom") {
        unpredictable("An implementation is free to break a misaligned access any way, leading to unpredictable behavior when any part of the misaligned access causes an exception");
      }
    }
  }
}

